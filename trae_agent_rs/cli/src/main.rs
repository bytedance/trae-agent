// Copyright (c) 2025 ByteDance Ltd. and/or its affiliates
// SPDX-License-Identifier: MIT

use anyhow::Result;
use clap::Parser;
use owo_colors::OwoColorize;
use std::{collections::HashMap, path::PathBuf};
use trae_cli::{get_trae_agent_logo, tui::App};
use trae_core::{
    agent::base_agent::{Agent, AgentExecution, BaseAgent},
    config::{ModelConfig, ModelProvider},
    llm::LLMClient,
    trae::TraeAgent,
};

fn show_welcome_message() {
    // Title to display
    let text = get_trae_agent_logo();

    println!("{}", text);

    println!();
    println!(
        "ðŸ¤– {} - Intelligent coding assistant",
        "Trae Agent".bright_cyan()
    );
    println!("ðŸ’¡ Version: {}", env!("CARGO_PKG_VERSION").bright_green());
    println!();
}

#[derive(Parser)]
#[command(
    name = "trae-agent",
    about = "Trae Agent - Intelligent coding assistant",
    version = env!("CARGO_PKG_VERSION"),
    author = "ByteDance"
)]
struct Cli {
    /// Start interactive mode (default behavior)
    #[arg(short, long)]
    interactive: bool,

    /// Run a single task without interactive mode
    #[arg(short, long)]
    run: Option<String>,

    /// Enable verbose logging
    #[arg(short, long, global = true)]
    verbose: bool,

    /// Configuration file path
    #[arg(short, long, global = true)]
    config: Option<PathBuf>,

    /// Working directory for the agent
    #[arg(short, long, global = true, default_value = ".")]
    workspace: PathBuf,

    /// Model provider to use (openai, anthropic, etc.)
    #[arg(short, long, global = true, default_value = "openai")]
    provider: String,

    /// Model name to use
    #[arg(long, global = true, default_value = "gpt-4")]
    model: String,
}

#[tokio::main]
async fn main() -> Result<()> {
    let cli = Cli::parse();

    // Initialize logging
    env_logger::Builder::new()
        .filter_level(if cli.verbose {
            log::LevelFilter::Debug
        } else {
            log::LevelFilter::Info
        })
        .init();

    // Check if we have a run command
    if let Some(task) = cli.run {
        // Single run mode
        handle_run(task, cli.workspace, cli.provider, cli.model).await?;
    } else {
        // Interactive mode (default)
        handle_interactive(cli.workspace, cli.provider, cli.model).await?;
    }

    Ok(())
}

async fn handle_interactive(
    workspace: PathBuf,
    provider: String,
    model: String,
) -> Result<()> {
    println!("ðŸš€ Starting interactive session...");
    println!(
        "ðŸ“ Workspace: {}",
        workspace.display().to_string().bright_blue()
    );
    println!(
        "ðŸŽ¯ Provider: {}, Model: {}",
        provider.bright_green(),
        model.bright_yellow()
    );

    // Create and run the TUI application
    let mut app = App::new(provider, model, workspace)?;
    app.run().await?;

    Ok(())
}

async fn handle_run(
    task: String,
    workspace: PathBuf,
    provider: String,
    model: String,
) -> Result<()> {
    show_welcome_message();

    println!("ðŸš€ Executing task: {}", task.bright_white());
    println!(
        "ðŸ“ Workspace: {}",
        workspace.display().to_string().bright_blue()
    );
    println!(
        "ðŸŽ¯ Provider: {}, Model: {}",
        provider.bright_green(),
        model.bright_yellow()
    );

    // Create model configuration
    let api_key = match provider.as_str() {
        "openai" => std::env::var("OPENAI_API_KEY")
            .or_else(|_| std::env::var("API_KEY"))
            .unwrap_or_default(),
        "anthropic" => std::env::var("ANTHROPIC_API_KEY").unwrap_or_default(),
        "azure" => std::env::var("AZURE_API_KEY").unwrap_or_default(),
        _ => {
            eprintln!(
                "âŒ Unknown provider: {}. Supported providers: openai, anthropic, azure",
                provider
            );
            std::process::exit(1);
        }
    };

    if api_key.is_empty() {
        eprintln!("âŒ API key not found. Please set the appropriate environment variable:");
        match provider.as_str() {
            "openai" => eprintln!("   export OPENAI_API_KEY=your_key_here"),
            "anthropic" => eprintln!("   export ANTHROPIC_API_KEY=your_key_here"),
            "azure" => eprintln!("   export AZURE_API_KEY=your_key_here"),
            _ => {}
        }
        std::process::exit(1);
    }

    let base_url = match provider.as_str() {
        "openai" => Some("https://api.openai.com/v1".to_string()),
        "anthropic" => Some("https://api.anthropic.com".to_string()),
        "azure" => std::env::var("AZURE_BASE_URL").ok(),
        _ => None,
    };

    let model_provider = ModelProvider::new(provider.clone()).with_api_key(api_key);

    let model_provider = if let Some(url) = base_url {
        model_provider.with_base_url(url)
    } else {
        model_provider
    };

    let model_config = ModelConfig::new(model, model_provider)
        .with_max_tokens(4096)
        .with_temperature(0.1);

    println!("âš™ï¸ Model config: {:?}", model_config);

    // Create and initialize the agent
    match create_and_run_agent(model_config, workspace, task).await {
        Ok(_) => {
            println!("âœ… Task completed successfully!");
        }
        Err(e) => {
            eprintln!("âŒ Task failed: {}", e);
            std::process::exit(1);
        }
    }

    Ok(())
}

async fn create_and_run_agent(
    model_config: ModelConfig,
    workspace: PathBuf,
    task: String,
) -> Result<()> {
    println!("ðŸ”§ Creating agent...");

    // Create LLM client
    let llm_client = LLMClient::new(model_config.clone())?;

    // Create base agent
    let base_agent = BaseAgent::new(
        "".to_string(), // Empty task initially
        AgentExecution::new("".to_string(), None),
        llm_client,
        10, // max_step
        model_config,
        None, // tools will be set in new_task
        vec![],
    );

    // Create TraeAgent
    let mut agent = TraeAgent::new(base_agent, Some(workspace.to_string_lossy().to_string()));

    println!("ðŸŽ¯ Initializing task...");

    // Setup task arguments
    let mut args = HashMap::new();
    args.insert(
        "project_path".to_string(),
        workspace.to_string_lossy().to_string(),
    );
    args.insert("issue".to_string(), task.clone());

    // Initialize the task
    agent
        .new_task(task, Some(args), None)
        .map_err(|e| anyhow::anyhow!("Failed to initialize task: {:?}", e))?;

    println!("ðŸš€ Running agent...");

    // Run the agent
    let result = agent
        .run()
        .await
        .map_err(|e| anyhow::anyhow!("Agent execution failed: {}", e))?;

    println!("ðŸ“‹ Execution result: {:?}", result);

    Ok(())
}